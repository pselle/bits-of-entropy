<html>
  <head>
    <meta charset="utf-8">
    <title>Calculating Shannon Entropy</title>
    <link href="src/style.css" rel="stylesheet">
  </head>
  <body>
    <h1>Bits of Entropy</h1>
    <p>Shannon entropy was introduced by Claude E. Shannon in the 1948 paper "A Mathematical Theory of Communication". It's this equation:</p>
    <img src="src/public/entropyequation.jpg" />

    <p>Shannon entropy is the "average minimum number of bits needed to encode a string of symbols, based on the frequency of the symbols" (<a href="http://www.bearcave.com/misl/misl_tech/wavelets/compression/shannon.html" alt="Bear Cave article on Shannon entropy">source</a>).</p>

    <p>Imagine that you and your friends have decided to communicate in an emoji-only protocol. Each emoji represents a message. But some emoji are more commonly used than others â€“ ex. ğŸ˜€&nbsp; appears more in messages among your friends than ğŸ­. Another way of saying that is that has a higher probability of appearing in a given message than ğŸ­.</p>

    <p>Since ğŸ˜€ &nbsp; is more commonly used than ğŸ­, it makes sense to be smarter about encoding your messages such that ğŸ˜€ takes up less space than ğŸ­. What Shannon entropy calculates is the average minimum number of bits needed to encode a message, since we want to send our messages in the least amount of space possible, because yay, then we can send more messages and/or be efficient and stuff!

    <p>So in this Shannon entropy calculator, our <strong>symbols</strong> are emoji, and each member of the set has a different probability of being represented in a given message. As you change the probabilities, make sure they add up to 1 (100%) since otherwise your probability distribution is incomplete.</p>

    <p>You have 5 emoji to start with, and you can add and remove emoji to see how the number of items in the set impacts the entropy!</p>
    <div id="app" />
    <script src="src/public/bundle.js" type="text/javascript"></script>
  </body>
</html>